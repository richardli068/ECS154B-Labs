\documentclass[11pt]{report}  
\usepackage{geometry}                		
\geometry{letterpaper, margin=0.8in}                
\usepackage[parfill]{parskip}   
\usepackage{pdfpages}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{wrapfig}			
\usepackage{listings}	
\usepackage{amssymb}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{amsmath}

% page header/footer setup
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}
\fancyhead[L]{Lab5}


\title{Lab5}
\author{
	Anton Pupkov
	\and
	Richard Li
}

\date{March 9, 2018}

\begin{document}
\maketitle

% Question 1
\section*{Question 1: Miss ratio and cache size}
  
  % insert image 1
  \begin{figure}[H]
	  \centering
		\includegraphics[width=6in]{q1.png}
		\caption{Direct Mapped Miss Ratio}
	\end{figure}
	% end insert image1
	
	% a
  \subsection*{1a}
    The miss ratio behaves like function $\frac{1}{1+\exp^x}$. As the cache size increases, it starts off decreasing slowly, followed by a period of fast decrementing, then reduces the rate of decreasing and ends with a slope of 0. The miss ratio decreases at first because as the cache size increases, more data can be put into the cache; thus less misses occurs. It eventually stops decreasing because the cache at 1MB could contain all data from all cache accesses.
    
  % b  
  \subsection*{1b}
    The minimum miss ratio is $\frac{1}{120000}$. It is not 0 because in the best scenario the first address will miss because the cache is empty and all 119999 subsequent addresses will hit.




% Question 2

\section*{Question 2: Set-associative caches}

  % insert image 2_1
  \begin{figure}[H]
	  \centering
		\includegraphics[width=4.5in]{q2.png}
		\caption{Set Associative Cache Miss Ratio}
	\end{figure}
	% end insert image 2_1
	
	% insert image 2_2
  \begin{figure}[H]
	  \centering
		\includegraphics[width=4.5in]{q2_2.png}
		\caption{Set Associative Execution Time}
	\end{figure}
	% end insert image 2_2
	
	% a
  \subsection*{2a}
    Execution time and miss ratio are proportional for the SA cache. For both SA and DM caches, since every request is done sequentially, the execution time is strongly related to the miss ratio. Thus, the execution time and miss ratio are proportional for DM caches too.
    
  % b  
  \subsection*{2b}
    It is not always worth it to increase the associativity. As seen in the figures, from 1 to 4 ways, the miss ratio actually increases as the associativity increases. However, when associativity is greater than 8, the execution time starts to decrease by roughly $base + s \times2000$, for each step s, as associativity increases. Since the decrease function is linear, one can argue that in order to maximize productivity and performance, there is a point to stop increasing associativity. However, the point where to stop could vary in different scenarios.




% Question 3
\section*{Question: Non-blocking cache performance}

  % insert image 3
  \begin{figure}[H]
	  \centering
		\includegraphics[width=6in]{q3.png}
		\caption{Non-blocking Cache Execution Time}
	\end{figure}
	% end insert image 3

  % a
  \subsection*{3a}
    No, it is not linear because memory access takes time. If memory access is instant, increasing MSHR would linearly increase the performance to a degree.
    
  %b
  \subsection*{3b (Extra)}
    Suppose the miss rate for a Non-blocking cache is n. The system requires $0.667\;cycles/request$ and $15\;cycle/respond$. Then
    \begin{align*}
      \frac{2\;cycle}{3\;request} \times \dfrac{1\;request}{n\;miss} = \frac{2\;cycle}{3n\;miss}
    \end{align*}
    Little's Law states,
    \begin{equation}
      L = \lambda\mathcal{W}
    \end{equation}
    where $L$ is the number of MSHR, $\lambda$ is the number of miss per cycle, and $\mathcal{W}$ is the time required per miss.\\
    Therefore, the least number of MSHR required to minimize stalling is
    \begin{align*}
      L &= \frac{3n\;miss}{2\;cycle} \times \frac{15\;cycle}{1\;miss} \\
        &= 22.5n
    \end{align*}
    If the miss ratio is 0.6, the least number of MSHR required to minimize stalling would be $22.5 \times 0.6 \approx 14$.
  
  
% Question 4
\section*{Question 4: Real systems}

  % insert image 4
  \begin{figure}[H]
	  \centering
		\includegraphics[width=6in]{q4.png}
		\caption{Performance of Macbook Pro on 1024 $\times$ 1024 Matrix Multiplication}
	\end{figure}
	% end insert image 4
	
  % a
  \subsection*{4a}
    mm64 performs the best on our testing computer. Smaller block sizes are slower because less data is brought into the cache on each iteration of the for loop, which mean less spatial locality.
    
  % b
  \subsection*{4b}
    We think a smaller block size would perform better on a smart phone. Since a smart phone has a smaller cache size, the data needed for an iteration might not fit into the top level cache, meaning fetching from the second level is constantly required. Thus, a smaller block size would ensure maximum benefit from spatial locality because data is brought into the top level cache.


\end{document}
